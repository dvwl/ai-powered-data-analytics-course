{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9623879b",
   "metadata": {},
   "source": [
    "# ü§ñ AI-Powered Data Analytics Demo\n",
    "\n",
    "Welcome to the future of data analysis! This demo showcases how AI can accelerate and enhance your data analytics workflows.\n",
    "\n",
    "## üìã What You'll Learn\n",
    "- Set up AI tools for data work (OpenAI/Azure OpenAI)\n",
    "- Use AI for automated data cleaning and exploration\n",
    "- Generate analysis code through natural language prompts\n",
    "- Create visualizations with AI assistance\n",
    "- Build comprehensive reports using AI\n",
    "- Understand best practices and limitations\n",
    "\n",
    "## ‚ö° Prerequisites\n",
    "- Python 3.7+\n",
    "- OpenAI API key (or Azure OpenAI access)\n",
    "- Basic understanding of data analysis concepts\n",
    "\n",
    "## üöÄ Let's Begin!\n",
    "Follow along as we transform how you approach data analytics with AI assistance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ea5385",
   "metadata": {},
   "source": [
    "## üîß Section 1: Setting Up AI Tools for Data Work\n",
    "\n",
    "Before we begin, we need to configure our environment for AI-assisted data analysis.\n",
    "\n",
    "### Step 1: Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2f95c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries (run this once)\n",
    "# !pip install openai pandas numpy matplotlib seaborn plotly python-dotenv\n",
    "\n",
    "# Import essential libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"OpenAI version: {openai.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40fa1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set up OpenAI client\n",
    "try:\n",
    "    # Option 1: Standard OpenAI API\n",
    "    if os.getenv('OPENAI_API_KEY'):\n",
    "        client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "        model_name = os.getenv('OPENAI_MODEL', 'gpt-4')\n",
    "        print(\"‚úÖ OpenAI API configured successfully!\")\n",
    "    \n",
    "    # Option 2: Azure OpenAI (uncomment if using Azure)\n",
    "    # elif os.getenv('AZURE_OPENAI_KEY'):\n",
    "    #     client = openai.AzureOpenAI(\n",
    "    #         api_key=os.getenv('AZURE_OPENAI_KEY'),\n",
    "    #         api_version=os.getenv('AZURE_OPENAI_VERSION', '2024-02-15-preview'),\n",
    "    #         azure_endpoint=os.getenv('AZURE_OPENAI_ENDPOINT')\n",
    "    #     )\n",
    "    #     model_name = os.getenv('AZURE_OPENAI_MODEL', 'gpt-4')\n",
    "    #     print(\"‚úÖ Azure OpenAI configured successfully!\")\n",
    "    \n",
    "    else:\n",
    "        print(\"‚ùå No API key found! Please check your .env file.\")\n",
    "        print(\"Make sure you have either OPENAI_API_KEY or AZURE_OPENAI_KEY set.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error setting up AI client: {e}\")\n",
    "    print(\"Please check your API key and try again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bacadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for AI API calls\n",
    "def ask_ai(prompt, temperature=0.3, max_tokens=1000):\n",
    "    \"\"\"\n",
    "    Send a prompt to the AI and get a response\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert data analyst assistant. Provide clear, accurate, and actionable insights.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# Test the connection\n",
    "test_response = ask_ai(\"Say 'Hello! AI is ready for data analytics!' in a professional tone.\")\n",
    "print(\"ü§ñ AI Response:\", test_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e473dd6",
   "metadata": {},
   "source": [
    "## üéØ Section 2: First AI-Assisted Data Analysis Demo\n",
    "\n",
    "Let's dive into our first AI-assisted data analysis using the retail sales sample data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b7fd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the sample retail sales data\n",
    "df = pd.read_csv('retail-sales-sample.csv')\n",
    "\n",
    "print(\"üìä Dataset loaded successfully!\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nDataset Info:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\nBasic Statistics:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530772f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's ask AI to analyze our dataset structure\n",
    "dataset_info = f\"\"\"\n",
    "Dataset: Retail Sales Data\n",
    "Columns: {list(df.columns)}\n",
    "Shape: {df.shape}\n",
    "Data types: {df.dtypes.to_dict()}\n",
    "Sample data: {df.head(3).to_dict()}\n",
    "\"\"\"\n",
    "\n",
    "ai_analysis = ask_ai(f\"\"\"\n",
    "Analyze this retail sales dataset and provide:\n",
    "1. Key insights about the data structure\n",
    "2. Potential data quality issues to watch for\n",
    "3. 3 interesting business questions we could explore\n",
    "4. Recommended next steps for analysis\n",
    "\n",
    "Dataset info: {dataset_info}\n",
    "\"\"\")\n",
    "\n",
    "print(\"ü§ñ AI Analysis:\")\n",
    "print(ai_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931e2c4a",
   "metadata": {},
   "source": [
    "## üßπ Section 3: AI for Data Cleaning\n",
    "\n",
    "Now let's use AI to help us clean and prepare our data for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c54c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask AI to generate comprehensive data quality check code\n",
    "quality_check_prompt = f\"\"\"\n",
    "Create Python code to perform comprehensive data quality checks on this retail sales dataset:\n",
    "Columns: {list(df.columns)}\n",
    "Data types: {df.dtypes.to_dict()}\n",
    "\n",
    "Include checks for:\n",
    "1. Missing values\n",
    "2. Duplicate records\n",
    "3. Outliers in numerical columns\n",
    "4. Data type consistency\n",
    "5. Date format validation\n",
    "6. Categorical value validation\n",
    "\n",
    "Return only the Python code with comments.\n",
    "\"\"\"\n",
    "\n",
    "ai_code = ask_ai(quality_check_prompt, temperature=0.1)\n",
    "print(\"ü§ñ AI-Generated Data Quality Check Code:\")\n",
    "print(ai_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c58079d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's run our own data quality checks based on AI suggestions\n",
    "print(\"üìã Data Quality Assessment\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Missing Values Check\n",
    "print(\"1. Missing Values:\")\n",
    "missing_data = df.isnull().sum()\n",
    "print(missing_data[missing_data > 0] if missing_data.sum() > 0 else \"‚úÖ No missing values found\")\n",
    "\n",
    "# 2. Duplicate Records Check\n",
    "print(f\"\\n2. Duplicate Records: {df.duplicated().sum()}\")\n",
    "\n",
    "# 3. Date Validation\n",
    "print(f\"\\n3. Date Format Check:\")\n",
    "try:\n",
    "    df['date_parsed'] = pd.to_datetime(df['date'])\n",
    "    print(\"‚úÖ Date format is valid\")\n",
    "    print(f\"Date range: {df['date_parsed'].min()} to {df['date_parsed'].max()}\")\n",
    "except:\n",
    "    print(\"‚ùå Date format issues detected\")\n",
    "\n",
    "# 4. Numerical Outliers (using IQR method)\n",
    "print(f\"\\n4. Outlier Detection:\")\n",
    "numerical_cols = ['price', 'quantity', 'customer_satisfaction', 'return_rate']\n",
    "for col in numerical_cols:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    outliers = df[(df[col] < (Q1 - 1.5 * IQR)) | (df[col] > (Q3 + 1.5 * IQR))]\n",
    "    print(f\"  {col}: {len(outliers)} outliers detected\")\n",
    "\n",
    "# 5. Categorical Value Validation\n",
    "print(f\"\\n5. Categorical Data Summary:\")\n",
    "categorical_cols = ['category', 'region', 'sales_rep']\n",
    "for col in categorical_cols:\n",
    "    print(f\"  {col}: {df[col].nunique()} unique values\")\n",
    "    print(f\"    Values: {list(df[col].unique())}\")\n",
    "\n",
    "print(\"\\n‚úÖ Data quality assessment complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c1561c",
   "metadata": {},
   "source": [
    "## üîç Section 4: AI-Powered Data Exploration\n",
    "\n",
    "Let's use natural language to explore our data and generate insights!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892a04f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Natural Language Data Query Function\n",
    "def query_data_with_ai(question, dataframe=df):\n",
    "    \"\"\"\n",
    "    Ask questions about the data in natural language\n",
    "    \"\"\"\n",
    "    data_context = f\"\"\"\n",
    "    Dataset info:\n",
    "    - Shape: {dataframe.shape}\n",
    "    - Columns: {list(dataframe.columns)}\n",
    "    - Sample data: {dataframe.head(3).to_dict()}\n",
    "    - Data types: {dataframe.dtypes.to_dict()}\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    You are a data analyst. Answer this question about the retail sales dataset:\n",
    "    \n",
    "    Question: {question}\n",
    "    \n",
    "    {data_context}\n",
    "    \n",
    "    Provide:\n",
    "    1. A direct answer to the question\n",
    "    2. Python code to verify your answer (if applicable)\n",
    "    3. Any interesting insights or patterns you notice\n",
    "    \n",
    "    Be specific and actionable.\n",
    "    \"\"\"\n",
    "    \n",
    "    return ask_ai(prompt)\n",
    "\n",
    "# Let's try some natural language queries\n",
    "questions = [\n",
    "    \"Which region has the highest average sales?\",\n",
    "    \"What's the relationship between price and customer satisfaction?\",\n",
    "    \"Which sales rep is performing best?\",\n",
    "    \"Are there any seasonal patterns in the data?\"\n",
    "]\n",
    "\n",
    "for i, question in enumerate(questions, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"‚ùì Question {i}: {question}\")\n",
    "    print(\"ü§ñ AI Response:\")\n",
    "    response = query_data_with_ai(question)\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a517a7db",
   "metadata": {},
   "source": [
    "## üéØ Section 5: Prompt Engineering for Data Tasks\n",
    "\n",
    "Learn how to craft effective prompts for better AI assistance in data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1abb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt Engineering Examples\n",
    "\n",
    "# Example 1: Vague vs Specific Prompts\n",
    "print(\"üî¥ POOR PROMPT:\")\n",
    "poor_prompt = \"Analyze the data\"\n",
    "print(f\"Prompt: '{poor_prompt}'\")\n",
    "response = ask_ai(poor_prompt)\n",
    "print(\"Response:\", response[:200] + \"...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "print(\"\\nüü¢ GOOD PROMPT:\")\n",
    "good_prompt = f\"\"\"\n",
    "Analyze this retail sales dataset and provide:\n",
    "\n",
    "Context: E-commerce ice cream sales data with {df.shape[0]} records\n",
    "Goal: Identify top 3 business improvement opportunities\n",
    "\n",
    "Dataset summary:\n",
    "- Columns: {list(df.columns)}\n",
    "- Date range: {df['date'].min()} to {df['date'].max()}\n",
    "- Revenue metrics: price, quantity, customer_satisfaction\n",
    "\n",
    "Required output:\n",
    "1. Top 3 specific findings with supporting data\n",
    "2. Python code to verify each finding\n",
    "3. Business recommendations\n",
    "\n",
    "Focus on actionable insights that could increase revenue or customer satisfaction.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Prompt structure demonstrated above\")\n",
    "response = ask_ai(good_prompt)\n",
    "print(\"ü§ñ AI Response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccce35a",
   "metadata": {},
   "source": [
    "## üíª Section 6: AI for Code Generation\n",
    "\n",
    "Watch AI generate analysis code from natural language descriptions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa9e7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI Code Generator Function\n",
    "def generate_analysis_code(description):\n",
    "    \"\"\"\n",
    "    Generate Python code for data analysis tasks\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Generate Python pandas code for this analysis task:\n",
    "    \n",
    "    Task: {description}\n",
    "    \n",
    "    Dataset variable name: df\n",
    "    Available columns: {list(df.columns)}\n",
    "    \n",
    "    Requirements:\n",
    "    1. Include comments explaining each step\n",
    "    2. Use proper pandas/matplotlib syntax\n",
    "    3. Handle potential errors gracefully\n",
    "    4. Return only the Python code\n",
    "    \n",
    "    Code:\n",
    "    \"\"\"\n",
    "    \n",
    "    return ask_ai(prompt, temperature=0.1)\n",
    "\n",
    "# Example code generation requests\n",
    "tasks = [\n",
    "    \"Create a summary table showing average sales by region and category\",\n",
    "    \"Build a correlation matrix for numerical columns with visualization\",\n",
    "    \"Find the top 5 best-performing products by total revenue\",\n",
    "    \"Create a time series plot showing sales trends over time\"\n",
    "]\n",
    "\n",
    "for i, task in enumerate(tasks, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üìù Task {i}: {task}\")\n",
    "    print(\"\\nü§ñ Generated Code:\")\n",
    "    code = generate_analysis_code(task)\n",
    "    print(code)\n",
    "    print(\"\\n\" + \"-\"*40 + \" EXECUTING CODE \" + \"-\"*40)\n",
    "    \n",
    "    try:\n",
    "        # Note: In a real scenario, you'd want to be more careful about executing AI-generated code\n",
    "        exec(code)\n",
    "    except Exception as e:\n",
    "        print(f\"Error executing code: {e}\")\n",
    "        print(\"Code may need manual review and adjustment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf6c88d",
   "metadata": {},
   "source": [
    "## üìä Section 7: AI-Generated Visualizations\n",
    "\n",
    "Let AI suggest and create the perfect visualizations for your data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b115da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI Visualization Recommender\n",
    "def recommend_visualizations(analysis_goal):\n",
    "    \"\"\"\n",
    "    Get AI recommendations for the best visualizations\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    I want to analyze: {analysis_goal}\n",
    "    \n",
    "    Dataset context:\n",
    "    - Retail sales data with {df.shape[0]} records\n",
    "    - Columns: {list(df.columns)}\n",
    "    - Numerical: price, quantity, customer_satisfaction, return_rate\n",
    "    - Categorical: name, product, category, sales_rep, region\n",
    "    - Temporal: date\n",
    "    \n",
    "    Recommend:\n",
    "    1. The 2 best chart types for this analysis\n",
    "    2. Python code using matplotlib/seaborn/plotly\n",
    "    3. Why these charts are most effective\n",
    "    4. What insights they might reveal\n",
    "    \n",
    "    Include complete, executable Python code.\n",
    "    \"\"\"\n",
    "    \n",
    "    return ask_ai(prompt, temperature=0.2)\n",
    "\n",
    "# Example visualization requests\n",
    "viz_goals = [\n",
    "    \"Compare performance across different regions\",\n",
    "    \"Show the relationship between price and customer satisfaction\",\n",
    "    \"Identify sales trends over time\",\n",
    "    \"Display the distribution of return rates by category\"\n",
    "]\n",
    "\n",
    "for goal in viz_goals:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üéØ Analysis Goal: {goal}\")\n",
    "    print(\"\\nü§ñ AI Recommendation:\")\n",
    "    recommendation = recommend_visualizations(goal)\n",
    "    print(recommendation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e0a34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create some quick visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Regional Performance\n",
    "regional_sales = df.groupby('region').agg({\n",
    "    'quantity': 'sum',\n",
    "    'customer_satisfaction': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "axes[0,0].bar(regional_sales.index, regional_sales['quantity'])\n",
    "axes[0,0].set_title('Total Quantity Sold by Region')\n",
    "axes[0,0].set_ylabel('Quantity')\n",
    "\n",
    "# 2. Price vs Satisfaction\n",
    "axes[0,1].scatter(df['price'], df['customer_satisfaction'], alpha=0.6)\n",
    "axes[0,1].set_title('Price vs Customer Satisfaction')\n",
    "axes[0,1].set_xlabel('Price ($)')\n",
    "axes[0,1].set_ylabel('Customer Satisfaction')\n",
    "\n",
    "# 3. Sales Trends\n",
    "df['date_parsed'] = pd.to_datetime(df['date'])\n",
    "daily_sales = df.groupby('date_parsed')['quantity'].sum()\n",
    "axes[1,0].plot(daily_sales.index, daily_sales.values)\n",
    "axes[1,0].set_title('Sales Trends Over Time')\n",
    "axes[1,0].set_xlabel('Date')\n",
    "axes[1,0].set_ylabel('Quantity Sold')\n",
    "axes[1,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 4. Return Rate Distribution\n",
    "df.boxplot(column='return_rate', by='category', ax=axes[1,1])\n",
    "axes[1,1].set_title('Return Rate Distribution by Category')\n",
    "axes[1,1].set_xlabel('Category')\n",
    "axes[1,1].set_ylabel('Return Rate')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Visualizations created! AI can help you choose the right charts for your analysis goals.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534fc389",
   "metadata": {},
   "source": [
    "## üìà Section 8: AI for Statistical Analysis\n",
    "\n",
    "Use AI to choose appropriate statistical tests and interpret results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6031df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI Statistical Analysis Advisor\n",
    "def get_statistical_advice(research_question):\n",
    "    \"\"\"\n",
    "    Get AI advice on statistical analysis approach\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Research Question: {research_question}\n",
    "    \n",
    "    Dataset Context:\n",
    "    - Sample size: {df.shape[0]} records\n",
    "    - Variables: {list(df.columns)}\n",
    "    - Numerical variables: price, quantity, customer_satisfaction, return_rate\n",
    "    - Categorical variables: region, category, sales_rep\n",
    "    - Data appears to be retail sales transactions\n",
    "    \n",
    "    Provide:\n",
    "    1. Appropriate statistical test(s) to use\n",
    "    2. Assumptions to check before running the test\n",
    "    3. Python code to perform the analysis\n",
    "    4. How to interpret the results\n",
    "    5. Potential limitations or caveats\n",
    "    \n",
    "    Be specific and practical.\n",
    "    \"\"\"\n",
    "    \n",
    "    return ask_ai(prompt, temperature=0.1)\n",
    "\n",
    "# Example statistical questions\n",
    "stat_questions = [\n",
    "    \"Is there a significant difference in customer satisfaction between regions?\",\n",
    "    \"Does price have a statistically significant correlation with return rate?\",\n",
    "    \"Are sales representatives performing significantly differently from each other?\"\n",
    "]\n",
    "\n",
    "for question in stat_questions:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"‚ùì Research Question: {question}\")\n",
    "    print(\"\\nü§ñ AI Statistical Advice:\")\n",
    "    advice = get_statistical_advice(question)\n",
    "    print(advice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184a35ce",
   "metadata": {},
   "source": [
    "## üìù Section 9: Automated Reporting with AI\n",
    "\n",
    "Generate comprehensive analysis reports automatically!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656dd129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI Report Generator\n",
    "def generate_executive_report():\n",
    "    \"\"\"\n",
    "    Generate a comprehensive executive report using AI\n",
    "    \"\"\"\n",
    "    # Gather key statistics\n",
    "    total_revenue = (df['price'] * df['quantity']).sum()\n",
    "    avg_satisfaction = df['customer_satisfaction'].mean()\n",
    "    top_region = df.groupby('region')['quantity'].sum().idxmax()\n",
    "    best_product = df.groupby('product')['quantity'].sum().idxmax()\n",
    "    \n",
    "    report_data = f\"\"\"\n",
    "    Dataset Summary:\n",
    "    - Total Records: {df.shape[0]}\n",
    "    - Date Range: {df['date'].min()} to {df['date'].max()}\n",
    "    - Total Revenue: ${total_revenue:,.2f}\n",
    "    - Average Customer Satisfaction: {avg_satisfaction:.2f}/5.0\n",
    "    - Top Performing Region: {top_region}\n",
    "    - Best Selling Product: {best_product}\n",
    "    \n",
    "    Regional Performance:\n",
    "    {df.groupby('region').agg({'quantity': 'sum', 'customer_satisfaction': 'mean'}).to_string()}\n",
    "    \n",
    "    Category Analysis:\n",
    "    {df.groupby('category').agg({'quantity': 'sum', 'return_rate': 'mean'}).to_string()}\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Create an executive summary report for retail sales performance based on this data analysis:\n",
    "    \n",
    "    {report_data}\n",
    "    \n",
    "    Structure the report with:\n",
    "    1. Executive Summary (key findings)\n",
    "    2. Performance Highlights\n",
    "    3. Areas of Concern\n",
    "    4. Strategic Recommendations\n",
    "    5. Next Steps\n",
    "    \n",
    "    Write in a professional business tone suitable for C-level executives.\n",
    "    Include specific metrics and actionable insights.\n",
    "    \"\"\"\n",
    "    \n",
    "    return ask_ai(prompt, max_tokens=1500)\n",
    "\n",
    "# Generate the report\n",
    "print(\"üìä AUTOMATED EXECUTIVE REPORT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "report = generate_executive_report()\n",
    "print(report)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ Report generated automatically using AI!\")\n",
    "print(\"üí° This report can be customized, exported to PDF, or integrated into dashboards.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccc83d0",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Section 10: Best Practices and Limitations\n",
    "\n",
    "Understanding when and how to use AI effectively in data analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c19651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Practices for AI-Assisted Data Analysis\n",
    "\n",
    "print(\"üîç VERIFICATION STRATEGIES\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\"\"\n",
    "1. Always verify AI-generated code before executing\n",
    "2. Cross-check AI insights with manual analysis\n",
    "3. Use multiple AI queries for complex questions\n",
    "4. Validate statistical claims with domain knowledge\n",
    "5. Test AI code on subset of data first\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n‚úÖ WHEN TO USE AI:\")\n",
    "print(\"\"\"\n",
    "‚Ä¢ Exploratory data analysis and pattern discovery\n",
    "‚Ä¢ Code generation for routine tasks\n",
    "‚Ä¢ Documentation and report writing\n",
    "‚Ä¢ Statistical test selection guidance\n",
    "‚Ä¢ Visualization recommendations\n",
    "‚Ä¢ Data cleaning automation\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n‚ùå WHEN NOT TO USE AI:\")\n",
    "print(\"\"\"\n",
    "‚Ä¢ Critical business decisions without human review\n",
    "‚Ä¢ Sensitive or proprietary data analysis\n",
    "‚Ä¢ Complex statistical modeling without validation\n",
    "‚Ä¢ Final production code without testing\n",
    "‚Ä¢ Regulatory compliance reporting\n",
    "‚Ä¢ Domain-specific analysis requiring expertise\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nüõ°Ô∏è DATA PRIVACY CONSIDERATIONS:\")\n",
    "print(\"\"\"\n",
    "‚Ä¢ Remove or anonymize PII before AI analysis\n",
    "‚Ä¢ Check company policies on AI tool usage\n",
    "‚Ä¢ Consider data residency and storage policies\n",
    "‚Ä¢ Be aware of AI model training implications\n",
    "‚Ä¢ Use synthetic data for training and demos\n",
    "\"\"\")\n",
    "\n",
    "# Example: Verifying AI insights\n",
    "print(\"\\nüî¨ VERIFICATION EXAMPLE:\")\n",
    "print(\"AI claimed: 'North region has highest customer satisfaction'\")\n",
    "\n",
    "# Manual verification\n",
    "regional_satisfaction = df.groupby('region')['customer_satisfaction'].mean().sort_values(ascending=False)\n",
    "print(f\"\\nManual verification:\")\n",
    "print(regional_satisfaction)\n",
    "\n",
    "top_region = regional_satisfaction.index[0]\n",
    "print(f\"\\n‚úÖ Verified: {top_region} region has highest satisfaction ({regional_satisfaction.iloc[0]:.2f})\")\n",
    "\n",
    "print(\"\\nüí° Always validate AI claims with data!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üéì DEMO COMPLETE!\")\n",
    "print(\"You've learned how to:\")\n",
    "print(\"‚Ä¢ Set up AI tools for data analytics\")\n",
    "print(\"‚Ä¢ Use AI for data cleaning and exploration\")\n",
    "print(\"‚Ä¢ Generate code and visualizations with AI\")\n",
    "print(\"‚Ä¢ Create automated reports\")\n",
    "print(\"‚Ä¢ Apply best practices and limitations\")\n",
    "print(\"\\nReady to revolutionize your data analysis workflow! üöÄ\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
